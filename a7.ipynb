{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cacc00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pypdf2 python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b16ad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "file = PdfReader(\"Resume.pdf\")\n",
    "text = \"\"\n",
    "for page in file.pages:\n",
    "    text = page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb48a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5066ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aff25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7461445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "filtered_tokens = []\n",
    "for word in tokens:\n",
    "    if word in stop_words:\n",
    "        continue\n",
    "    filtered_tokens.append(word)\n",
    "\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ede776",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "stemmed_tokens = []\n",
    "for word in filtered_tokens:\n",
    "    stemmed_tokens.append(ps.stem(word))\n",
    "stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd9eef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = []\n",
    "for word in filtered_tokens:\n",
    "    lemmatized_tokens.append(lemmatizer.lemmatize(word))\n",
    "lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbd32c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f33aee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'he': np.float64(0.0),\n",
       "  'is': np.float64(0.017114745199010992),\n",
       "  'a': np.float64(0.017114745199010992),\n",
       "  'boy': np.float64(0.0),\n",
       "  'and': np.float64(0.0),\n",
       "  'she': np.float64(0.07701635339554948),\n",
       "  'girl': np.float64(0.0)},\n",
       " {'he': np.float64(0.0),\n",
       "  'has': np.float64(0.07701635339554948),\n",
       "  'two': np.float64(0.07701635339554948),\n",
       "  'childrens,': np.float64(0.07701635339554948),\n",
       "  'one': np.float64(0.017114745199010992),\n",
       "  'boy': np.float64(0.0),\n",
       "  'and': np.float64(0.0),\n",
       "  'girl': np.float64(0.0)}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "doc1 = \"He is a boy and she is a girl\"\n",
    "doc2 = \"He has two childrens, one boy and one girl\"\n",
    "\n",
    "docs = [doc1.lower().split(), doc2.lower().split()]\n",
    "\n",
    "def computeTf(doc):\n",
    "    tf = {}\n",
    "    n = len(doc)\n",
    "    for word in doc:\n",
    "        tf[word] = tf.get(word, 0) + 1\n",
    "    for word in doc:\n",
    "        tf[word] = tf[word] / n\n",
    "    return tf\n",
    "\n",
    "tf_docs = [computeTf(doc) for doc in docs]\n",
    "vocabulary = set(word for doc in docs for word in doc)\n",
    "\n",
    "def computeIdf(docs, vocabulary):\n",
    "    idf = {}\n",
    "    n = len(docs)\n",
    "    for word in vocabulary:\n",
    "        count = sum(1 for doc in docs if word in doc)\n",
    "        if count == 0:\n",
    "            idf[word] = 0\n",
    "        else:\n",
    "            idf[word] = np.log(n/count)\n",
    "    return idf\n",
    "\n",
    "idf = computeIdf(docs, vocabulary)\n",
    "\n",
    "def computeTfIdf(tf_docs, idf):\n",
    "    tfidf_docs = []\n",
    "    for tf_doc in tf_docs:\n",
    "        tfidf = {}\n",
    "        for word in tf_doc:\n",
    "            tfidf[word] = tf_doc[word] * idf[word]\n",
    "        tfidf_docs.append(tfidf)\n",
    "    return tfidf_docs\n",
    "\n",
    "computeTfIdf(tf_docs, idf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
